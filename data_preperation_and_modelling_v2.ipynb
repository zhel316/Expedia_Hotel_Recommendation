{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-28T13:36:13.994868Z","iopub.status.busy":"2023-05-28T13:36:13.994355Z","iopub.status.idle":"2023-05-28T13:36:14.004352Z","shell.execute_reply":"2023-05-28T13:36:14.003293Z","shell.execute_reply.started":"2023-05-28T13:36:13.994836Z"},"trusted":true},"outputs":[],"source":["import time\n","import argparse\n","import pickle\n","import os\n","import gc\n","\n","import pandas\n","import numpy as np\n","import lightgbm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-28T13:36:14.084121Z","iopub.status.busy":"2023-05-28T13:36:14.083484Z","iopub.status.idle":"2023-05-28T13:36:14.148404Z","shell.execute_reply":"2023-05-28T13:36:14.147498Z","shell.execute_reply.started":"2023-05-28T13:36:14.084088Z"},"trusted":true},"outputs":[],"source":["# Load the dataset\n","def load_data(file_path):\n","    gc.collect()\n","    print(\"Started loading data from file {}\".format(file_path))\n","    orig_data = pandas.read_csv(file_path)\n","    print(\"Finished loading data....\")\n","    return orig_data\n","\n","# Add more date features from 'date_time' column\n","def add_date_features(\n","    in_data, datetime_key=\"date_time\", features=[\"month\", \"hour\", \"dayofweek\"]\n","):\n","    dates = pandas.to_datetime(in_data[datetime_key])\n","    for feature in features:\n","        if feature == \"month\":\n","            in_data[\"month\"] = dates.dt.month\n","        elif feature == \"dayofweek\":\n","            in_data[\"dayofweek\"] = dates.dt.dayofweek\n","        elif feature == \"hour\":\n","            in_data[\"hour\"] = dates.dt.hour\n","\n","    return in_data\n","\n","# Normalize the features to get better performance\n","def normalize_features(input_df, group_key, target_column, take_log10=False):\n","\n","    # for numerical stability\n","    epsilon = 1e-4\n","    if take_log10:\n","        input_df[target_column] = np.log10(input_df[target_column] + epsilon)\n","    methods = [\"mean\", \"std\"]\n","\n","    df = input_df.groupby(group_key).agg({target_column: methods})\n","\n","    df.columns = df.columns.droplevel()\n","    col = {}\n","    for method in methods:\n","        col[method] = target_column + \"_\" + method\n","\n","    df.rename(columns=col, inplace=True)\n","    df_merge = input_df.merge(df.reset_index(), on=group_key)\n","    df_merge[target_column + \"_norm_by_\" + group_key] = (\n","        df_merge[target_column] - df_merge[target_column + \"_mean\"]\n","    ) / df_merge[target_column + \"_std\"]\n","    df_merge = df_merge.drop(labels=[col[\"mean\"], col[\"std\"]], axis=1)\n","\n","    gc.collect()\n","    return df_merge\n","\n","\n","# Add aggregated features\n","def aggregated_features_single_column(\n","    in_data,\n","    key_for_grouped_by=\"prop_id\",\n","    target_column=\"price_usd\",\n","    agg_methods=[\"mean\", \"median\", \"min\", \"max\"],\n","    transform_methods={\"mean\": [\"substract\"]},\n","):\n","    df = in_data.groupby(key_for_grouped_by).agg({target_column: agg_methods})\n","\n","    if isinstance(key_for_grouped_by, list):\n","        str_key_for_grouped_by = \"|\".join(key_for_grouped_by)\n","    else:\n","        str_key_for_grouped_by = key_for_grouped_by\n","\n","    df.columns = df.columns.droplevel()\n","    col = {}\n","    for method in agg_methods:\n","        col[method] = (\n","            method.upper() + str_key_for_grouped_by + target_column\n","        )\n","\n","    df.rename(columns=col, inplace=True)\n","\n","    in_data = in_data.merge(df.reset_index(), on=key_for_grouped_by)\n","    for method_name in transform_methods:\n","        for applying_function in transform_methods[method_name]:\n","            function_data = in_data[\n","                method_name.upper()\n","                + str_key_for_grouped_by\n","                + target_column\n","            ]\n","            column_data = in_data[target_column]\n","            if applying_function == \"substract\":\n","                result = column_data - function_data\n","            elif applying_function == \"divide\":\n","                result = column_data / function_data\n","            else:\n","                continue\n","            in_data[\n","                applying_function.upper()\n","                + target_column\n","                + method_name.upper()\n","            ] = result\n","    gc.collect()\n","\n","    return in_data\n","\n","\n","def drop_columns_with_missing_data(\n","    df,\n","    threshold,\n","    ignore_values=[\n","        \"visitor_hist_adr_usd\",\n","        \"visitor_hist_starrating\",\n","        \"srch_query_affinity_score\",\n","    ],\n","):\n","    columns_to_drop = []\n","\n","    for i in range(df.shape[1]):\n","        length_df = len(df)\n","        column_names = df.columns.tolist()\n","        number_nans = sum(df.iloc[:, i].isnull())\n","        if number_nans / length_df > threshold:\n","            if column_names[i] not in ignore_values:\n","                columns_to_drop.append(column_names[i])\n","\n","    print(\n","        \"Dropping columns {} because they miss more than {} of data.\".format(\n","            columns_to_drop, threshold\n","        )\n","    )\n","\n","    df_reduced = df.drop(labels=columns_to_drop, axis=1)\n","    print(\"Dropped columns {}\".format(columns_to_drop))\n","    return df_reduced\n","\n","# Data preperation\n","def preprocess_training_data(orig_data, kind=\"train\", use_ndcg_choices=False):\n","\n","    print(\"Preprocessing training data....\")\n","    gc.collect()\n","    data_for_training = orig_data\n","\n","    target_column = \"target\"\n","\n","    if kind == \"train\":\n","        conditions = [\n","            data_for_training[\"click_bool\"] == 1,\n","            data_for_training[\"booking_bool\"] == 1,\n","        ]\n","        choices = [1, 2]\n","        data_for_training[target_column] = np.select(conditions, choices, default=0)\n","\n","    threshold = 0.9\n","    data_for_training = add_date_features(data_for_training)\n","    data_for_training.drop(labels=[\"date_time\"], axis=1, inplace=True)\n","\n","    data_for_training = drop_columns_with_missing_data(data_for_training, threshold)\n","\n","    # do not normalize 2 times with take_log10\n","    data_for_training = normalize_features(\n","        data_for_training,\n","        group_key=\"srch_id\",\n","        target_column=\"price_usd\",\n","        take_log10=True,\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"prop_id\", target_column=\"price_usd\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_starrating\"\n","    )\n","\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training, \"prop_id\", \"price_usd\", [\"mean\"]\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_starrating\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_location_score2\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_location_score1\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_destination_id\",\n","        target_column=\"price_usd\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_review_score\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"promotion_flag\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","\n","    # NOTE: has to be done after aggregated_features_single_column\n","    data_for_training = data_for_training.sort_values(\"srch_id\")\n","\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_starrating\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_location_score2\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_location_score1\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_review_score\"\n","    )\n","\n","    gc.collect()\n","    if kind == \"train\":\n","        y = data_for_training[target_column].values\n","    else:\n","        y = None\n","\n","    training_set_only_metrics = [\"click_bool\", \"booking_bool\", \"gross_bookings_usd\"]\n","    columns_to_remove = [\n","        \"date_time\",\n","        \"target\",\n","        target_column,\n","    ] + training_set_only_metrics\n","    columns_to_remove = [\n","        c for c in columns_to_remove if c in data_for_training.columns.values\n","    ]\n","    data_for_training = data_for_training.drop(labels=columns_to_remove, axis=1)\n","    return data_for_training, y\n","\n","# Remove specific columns\n","def remove_columns(x1, ignore_column=[\"srch_id\", \"prop_id\", \"position\", \"random_bool\"]):\n","    ignore_column = [c for c in ignore_column if c in x1.columns.values]\n","    # print('Dropping columns: {}'.format(ignore_column))\n","    # ignore_column_numbers = [x1.columns.get_loc(x) for x in ignore_column]\n","    x1 = x1.drop(labels=ignore_column, axis=1)\n","    # print('Columns after dropping: {}'.format(x1.columns.values))\n","    return x1\n","\n","\n","# Access search and hotel property information as well as estimated location information in the same dataset.\n","def input_estimated_position(training_data, srch_id_dest_id_dict):\n","    training_data = training_data.merge(\n","        srch_id_dest_id_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"]\n","    )\n","    print(training_data.head())\n","    return training_data\n","\n","# Split the training data\n","def split_train_data(data_for_training, y, val_start=0, val_end=0):\n","\n","    x1 = pandas.concat([data_for_training[0:val_start], data_for_training[val_end:]])\n","    y1 = np.concatenate((y[0:val_start], y[val_end:]), axis=0)\n","    x2 = data_for_training[val_start:val_end]\n","    y2 = y[val_start:val_end]\n","\n","    srch_id_dest_id_dict = x1.loc[x1[\"random_bool\"] == 0]\n","\n","    # estimated position calculation\n","    srch_id_dest_id_dict = x1.loc[x1[\"random_bool\"] == 0]\n","    srch_id_dest_id_dict = x1.groupby([\"srch_destination_id\", \"prop_id\"]).agg(\n","        {\"position\": \"mean\"}\n","    )\n","    srch_id_dest_id_dict = srch_id_dest_id_dict.rename(\n","        index=str, columns={\"position\": \"estimated_position\"}\n","    ).reset_index()\n","    srch_id_dest_id_dict[\"srch_destination_id\"] = (\n","        srch_id_dest_id_dict[\"srch_destination_id\"].astype(str).astype(int)\n","    )\n","    srch_id_dest_id_dict[\"prop_id\"] = (\n","        srch_id_dest_id_dict[\"prop_id\"].astype(str).astype(int)\n","    )\n","    srch_id_dest_id_dict[\"estimated_position\"] = (\n","        1 / srch_id_dest_id_dict[\"estimated_position\"]\n","    )\n","    x1 = input_estimated_position(x1, srch_id_dest_id_dict)\n","    x2 = input_estimated_position(x2, srch_id_dest_id_dict)\n","\n","    groups = x1[\"srch_id\"].value_counts(sort=False).sort_index()\n","    eval_groups = x2[\"srch_id\"].value_counts(sort=False).sort_index()\n","    len(eval_groups), len(x2), len(x1), len(groups)\n","\n","    x1 = remove_columns(x1)\n","    x2 = remove_columns(x2)\n","    return (x1, x2, y1, y2, groups, eval_groups, srch_id_dest_id_dict)\n","\n","# Get the index list of categorical feature columns from the input DataFrame.\n","def get_categorical_column(x1):\n","    categorical_features = [\n","        \"day\",\n","        \"month\",\n","        \"prop_country_id\",\n","        \"site_id\",\n","        \"visitor_location_country_id\",\n","    ]\n","    categorical_features = [c for c in categorical_features if c in x1.columns.values]\n","    categorical_features_numbers = [x1.columns.get_loc(x) for x in categorical_features]\n","    return categorical_features_numbers\n","\n","\n","# Train the model using LGBGRanker\n","def train_model(\n","    x1, x2, y1, y2, groups, eval_groups, lr, method, output_dir, name_of_model=None\n","):\n","    if not name_of_model:\n","        name_of_model = str(int(time.time()))\n","\n","    categorical_features_numbers = get_categorical_column(x1)\n","    clf = lightgbm.LGBMRanker(\n","        objective=\"lambdarank\",\n","        metric=\"ndcg\",\n","        n_estimators=1680,\n","        learning_rate=lr,\n","        max_position=5,\n","        label_gain=[0, 1, 2],\n","        random_state=69,\n","        seed=69,\n","        boosting=method,\n","    )\n","    gc.collect()\n","\n","    print(\"Training on train set with columns: {}\".format(x1.columns.values))\n","    clf.fit(\n","        x1,\n","        y1,\n","        eval_set=[(x1, y1), (x2, y2)],\n","        eval_group=[groups, eval_groups],\n","        group=groups,\n","        eval_at=5,\n","        verbose=20,\n","        early_stopping_rounds=200,\n","        categorical_feature=categorical_features_numbers,\n","    )\n","    gc.collect()\n","    pickle.dump(clf, open(os.path.join(output_dir, \"model.dat\"), \"wb\"))\n","    return clf\n","\n","# Data prediction\n","def predict(name_of_model, test_data, srch_id_dest_id_dict, output_dir):\n","\n","    gc.collect()\n","\n","    model = pickle.load(open(os.path.join(output_dir, \"model.dat\"), \"rb\"))\n","\n","    test_data = test_data.copy()\n","    test_data = input_estimated_position(test_data, srch_id_dest_id_dict)\n","\n","    test_data_srch_id_prop_id = test_data[[\"srch_id\", \"prop_id\"]]\n","\n","    test_data = remove_columns(test_data)\n","\n","    categorical_features_numbers = get_categorical_column(test_data)\n","\n","    print(\"Predicting on train set with columns: {}\".format(test_data.columns.values))\n","    kwargs = {}\n","    kwargs = {\"categorical_feature\": categorical_features_numbers}\n","\n","    predictions = model.predict(test_data, **kwargs)\n","    test_data_srch_id_prop_id[\"prediction\"] = predictions\n","    del test_data\n","    gc.collect()\n","\n","    test_data_srch_id_prop_id = test_data_srch_id_prop_id.sort_values(\n","        [\"srch_id\", \"prediction\"], ascending=False\n","    )\n","    print(\"Saving predictions into submission.csv\")\n","    test_data_srch_id_prop_id[[\"srch_id\", \"prop_id\"]].to_csv(\n","        os.path.join(output_dir, \"submission.csv\"), index=False\n","    )\n","    \n","def run(train_csv, test_csv, output_dir):\n","    name_of_model = str(int(time.time()))\n","\n","    training_data = load_data(train_csv)\n","    training_data, y = preprocess_training_data(training_data)\n","\n","    method = \"dart\"\n","    validation_num = 150000\n","    lr = 0.12\n","    # for i in range(0, int(len(training_data.index) / validation_num)): # enable for cross-validation\n","    for i in range(0, 1):\n","        val_start = i * validation_num\n","        val_end = (i + 1) * validation_num\n","        x1, x2, y1, y2, groups, eval_groups, srch_id_dest_id_dict = split_train_data(\n","            training_data, y, val_start, val_end\n","        )\n","        model = train_model(\n","            x1, x2, y1, y2, groups, eval_groups, lr, method, output_dir, name_of_model\n","        )\n","        test_data = load_data(test_csv)\n","        test_data, _ = preprocess_training_data(test_data, kind=\"test\")\n","        predict(name_of_model, test_data, srch_id_dest_id_dict, output_dir)\n","        print(\"Submit the predictions file submission.csv to kaggle\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-28T13:36:14.151462Z","iopub.status.busy":"2023-05-28T13:36:14.150781Z","iopub.status.idle":"2023-05-28T13:36:14.167085Z","shell.execute_reply":"2023-05-28T13:36:14.165866Z","shell.execute_reply.started":"2023-05-28T13:36:14.151430Z"},"trusted":true},"outputs":[],"source":["train_csv = \"/kaggle/input/vu-dmt-assigment-2-2023/training_set_VU_DM.csv\"\n","test_csv = \"/kaggle/input/vu-dmt-assigment-2-2023/test_set_VU_DM.csv\"\n","output_dir = \"/kaggle/working/\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-28T13:36:14.168912Z","iopub.status.busy":"2023-05-28T13:36:14.168587Z","iopub.status.idle":"2023-05-28T14:33:53.840889Z","shell.execute_reply":"2023-05-28T14:33:53.838005Z","shell.execute_reply.started":"2023-05-28T13:36:14.168885Z"},"trusted":true},"outputs":[],"source":["run(train_csv, test_csv, output_dir)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":5072532,"sourceId":47720,"sourceType":"competition"}],"dockerImageVersionId":30497,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
